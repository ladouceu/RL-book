{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from rl.markov_decision_process import FiniteMarkovDecisionProcess\n",
    "from rl.markov_decision_process import FinitePolicy, StateActionMapping\n",
    "from rl.markov_process import FiniteMarkovProcess, FiniteMarkovRewardProcess\n",
    "from rl.distribution import Categorical, Constant\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class FrogState:\n",
    "    position: int \n",
    "        \n",
    "        \n",
    "FrogMapping = StateActionMapping[FrogState, int]\n",
    "        \n",
    "        \n",
    "class FrogCroakMDP(FiniteMarkovDecisionProcess[FrogState, int]):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        num_lilypads: int,\n",
    "    \n",
    "    ):\n",
    "        self.num_lilypads: int = num_lilypads\n",
    "        super().__init__(self.get_action_transition_reward_map())\n",
    "        \n",
    "    def get_action_transition_reward_map(self) -> FrogMapping:\n",
    "        d: Dict[FrogState, Dict[int,Constant[Tuple[FrogState, int]]]] = {}\n",
    "        \n",
    "        # set the terminal states\n",
    "        d[0] = None\n",
    "        d[self.num_lilypads] = None\n",
    "        \n",
    "        # loop through all non-terminal states and generate the mapping\n",
    "        for s in range(1, self.num_lilypads):\n",
    "            d1: Dict[int,Categorical[Tuple[FrogState, int]]] = {}\n",
    "                            \n",
    "            # Action A is represented by True\n",
    "            # Action B is represented by False\n",
    "            d_actionA: Dict[Tuple[FrogState, int]] = {}\n",
    "            d_actionB: Dict[Tuple[FrogState, int]] = {}\n",
    "            for s_prime in range(0, self.num_lilypads+1):\n",
    "                # set the reward variable\n",
    "                if (s_prime == 0):\n",
    "                    reward = -1\n",
    "                elif (s_prime == self.num_lilypads):\n",
    "                    reward = 1\n",
    "                else:\n",
    "                    reward = 0\n",
    "                \n",
    "                # set the dict for action A\n",
    "                if (s == s_prime-1):\n",
    "                    d_actionA[(FrogState(s_prime),reward)] = s/self.num_lilypads\n",
    "                elif (s == s_prime+1):\n",
    "                    d_actionA[(FrogState(s_prime),reward)] = 1-s/self.num_lilypads\n",
    "                else:\n",
    "                    d_actionA[(FrogState(s_prime),reward)] = 0 \n",
    "                \n",
    "                # set the dict for action B\n",
    "                if (s != s_prime):\n",
    "                    d_actionB[(FrogState(s_prime),reward)] = 1/self.num_lilypads\n",
    "                else:\n",
    "                    d_actionB[(FrogState(s_prime),reward)] = 0\n",
    "                \n",
    "            d1[1]  = Categorical(d_actionA)\n",
    "            d1[0] = Categorical(d_actionB)\n",
    "            \n",
    "            d[s] = d1\n",
    "            \n",
    "        return d\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MDP Transition Map\n",
      "------------------\n",
      "0 is a Terminal State\n",
      "3 is a Terminal State\n",
      "From State 1:\n",
      "  With Action 1:\n",
      "    To [State FrogState(position=0) and Reward -1.000] with Probability 0.667\n",
      "    To [State FrogState(position=1) and Reward 0.000] with Probability 0.000\n",
      "    To [State FrogState(position=2) and Reward 0.000] with Probability 0.333\n",
      "    To [State FrogState(position=3) and Reward 1.000] with Probability 0.000\n",
      "  With Action 0:\n",
      "    To [State FrogState(position=0) and Reward -1.000] with Probability 0.333\n",
      "    To [State FrogState(position=1) and Reward 0.000] with Probability 0.000\n",
      "    To [State FrogState(position=2) and Reward 0.000] with Probability 0.333\n",
      "    To [State FrogState(position=3) and Reward 1.000] with Probability 0.333\n",
      "From State 2:\n",
      "  With Action 1:\n",
      "    To [State FrogState(position=0) and Reward -1.000] with Probability 0.000\n",
      "    To [State FrogState(position=1) and Reward 0.000] with Probability 0.333\n",
      "    To [State FrogState(position=2) and Reward 0.000] with Probability 0.000\n",
      "    To [State FrogState(position=3) and Reward 1.000] with Probability 0.667\n",
      "  With Action 0:\n",
      "    To [State FrogState(position=0) and Reward -1.000] with Probability 0.333\n",
      "    To [State FrogState(position=1) and Reward 0.000] with Probability 0.333\n",
      "    To [State FrogState(position=2) and Reward 0.000] with Probability 0.000\n",
      "    To [State FrogState(position=3) and Reward 1.000] with Probability 0.333\n",
      "\n",
      "Policy Map\n",
      "----------\n",
      "For State FrogState(position=0):\n",
      "  Do Action 0 with Probability 1.000\n",
      "For State FrogState(position=1):\n",
      "  Do Action 0 with Probability 1.000\n",
      "For State FrogState(position=2):\n",
      "  Do Action 0 with Probability 1.000\n",
      "For State FrogState(position=3):\n",
      "  Do Action 0 with Probability 1.000\n",
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-8b6e454d23f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mimplied_mrp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFiniteMarkovRewardProcess\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mFrogState\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mfrog_mdp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_finite_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfdp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Implied MP Transition Map\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"--------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/sf_vm_shared_folder/RL-book/rl/markov_decision_process.py\u001b[0m in \u001b[0;36mapply_finite_policy\u001b[0;34m(self, policy)\u001b[0m\n\u001b[1;32m    278\u001b[0m                     \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m                 \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mactions\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_action\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/sf_vm_shared_folder/RL-book/rl/markov_decision_process.py\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mFiniteDistribution\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 1"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    user_num_lilypads = 3 # or 6 or 9\n",
    "    \n",
    "    user_gamma = 0.5\n",
    "    \n",
    "    frog_mdp: FiniteMarkovDecisionProcess[FrogState, int] =\\\n",
    "        FrogCroakMDP(user_num_lilypads)\n",
    "    \n",
    "    print(\"MDP Transition Map\")\n",
    "    print(\"------------------\")\n",
    "    print(frog_mdp)\n",
    "\n",
    "    fdp: FinitePolicy[FrogState, int] = FinitePolicy(\n",
    "        {FrogState(position):\n",
    "         Constant(0) for position in\n",
    "         range(0, user_num_lilypads+1)}\n",
    "    )\n",
    " \n",
    "    print(\"Policy Map\")\n",
    "    print(\"----------\")\n",
    "    print(fdp)\n",
    "\n",
    "    implied_mrp: FiniteMarkovRewardProcess[FrogState] =\\\n",
    "        frog_mdp.apply_finite_policy(fdp)\n",
    "    print(\"Implied MP Transition Map\")\n",
    "    print(\"--------------\")\n",
    "    print(FiniteMarkovProcess(implied_mrp.transition_map))\n",
    "\n",
    "    print(\"Implied MRP Transition Reward Map\")\n",
    "    print(\"---------------------\")\n",
    "    print(implied_mrp)\n",
    "\n",
    "    print(\"Implied MP Stationary Distribution\")\n",
    "    print(\"-----------------------\")\n",
    "    implied_mrp.display_stationary_distribution()\n",
    "    print()\n",
    "\n",
    "    print(\"Implied MRP Reward Function\")\n",
    "    print(\"---------------\")\n",
    "    implied_mrp.display_reward_function()\n",
    "    print()\n",
    "\n",
    "    print(\"Implied MRP Value Function\")\n",
    "    print(\"--------------\")\n",
    "    implied_mrp.display_value_function(gamma=user_gamma)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
