{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Sequence, Set, Mapping, Dict, Callable, Optional\n",
    "from dataclasses import dataclass\n",
    "from operator import itemgetter\n",
    "from rl.distribution import Categorical, Choose, Constant\n",
    "from rl.markov_decision_process import FiniteMarkovDecisionProcess\n",
    "from rl.markov_decision_process import StateActionMapping\n",
    "from rl.markov_decision_process import FinitePolicy\n",
    "from rl.dynamic_programming import value_iteration_result, V\n",
    "\n",
    "\n",
    "def eps_greedy_action(\n",
    "    nt_state: Cell,\n",
    "    q: Mapping[Cell, Mapping[Move, float]],\n",
    "    epsilon: float) -> Move:\n",
    "    action_values: Mapping[Move, float] = q[nt_state]\n",
    "    return Categorical({a: epsilon / len(action_values) +(1 - epsilon if a == max(action_values.items(), key=itemgetter(1))[0] else 0.)\n",
    "         for a in action_values}).sample()\n",
    "\n",
    "def sarsa(\n",
    "    states_actions_dict: Mapping[S, Optional[Set[A]]],\n",
    "    sample_func: Callable[[S, A], Tuple[S, float]],\n",
    "    episodes: int = 10000,\n",
    "    step_size: float = 0.01) -> Tuple[V[S], FinitePolicy[S, A]]:\n",
    "\n",
    "    q: Dict[Cell, Dict[A, float]] = \\\n",
    "        {s: {a: 0. for a in actions} for s, actions in\n",
    "         states_actions_dict.items() if actions is not None}\n",
    "    nt_states: Set[S] = {s for s in q}\n",
    "    uniform_states: Choose[S] = Choose(nt_states)\n",
    "    for episode_num in range(episodes):\n",
    "        epsilon: float = 1.0 / (episode_num + 1)\n",
    "        state: S = uniform_states.sample()\n",
    "        action: A = eps_greedy_action(state, q, epsilon)\n",
    "        while(True): # loop until the episode terminates\n",
    "            tmp = sample_func(state, action)\n",
    "            next_state: S = tmp[0]\n",
    "            reward: float = tmp[1]\n",
    "            if(q[next_state] is None):\n",
    "                q[state][action] += step_size * (reward - q[state][action])\n",
    "                break\n",
    "            else:\n",
    "                next_action: A = eps_greedy_action(next_state, q, epsilon)\n",
    "                q[state][action] += step_size * (reward + q[next_state][next_action] - q[state][action])\n",
    "                action = next_action\n",
    "                state = next_state\n",
    "\n",
    "    vf_dict: V[S] = {s: max(d.values()) for s, d in q.items()}\n",
    "    policy: FinitePolicy[S, A] = FinitePolicy(\n",
    "        {s: Constant(max(d.items(), key=itemgetter(1))[0])\n",
    "         for s, d in q.items()}\n",
    "    )\n",
    "    return (vf_dict, policy)\n",
    "\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
