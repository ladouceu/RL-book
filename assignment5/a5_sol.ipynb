{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Approximate dynamic programming algorithms are variations on\n",
    "dynamic programming algorithms that can work with function\n",
    "approximations rather than exact representations of the process's\n",
    "state space.\n",
    "\n",
    "'''\n",
    "\n",
    "from typing import Iterator, Mapping, Tuple, TypeVar, Sequence, List\n",
    "from operator import itemgetter\n",
    "import numpy as np\n",
    "\n",
    "from rl.distribution import Distribution, Constant, Choose\n",
    "from rl.function_approx import FunctionApprox\n",
    "from rl.iterate import iterate\n",
    "from rl.markov_process import (FiniteMarkovRewardProcess, MarkovRewardProcess,\n",
    "                               RewardTransition)\n",
    "from rl.markov_decision_process import (FiniteMarkovDecisionProcess, Policy, FinitePolicy,\n",
    "                                        MarkovDecisionProcess,\n",
    "                                        StateActionMapping)\n",
    "\n",
    "S = TypeVar('S')\n",
    "A = TypeVar('A')\n",
    "\n",
    "# A representation of a value function for a finite MDP with states of\n",
    "# type S\n",
    "V = Mapping[S, float]\n",
    "\n",
    "\n",
    "def evaluate_finite_mrp(\n",
    "        mrp: FiniteMarkovRewardProcess[S],\n",
    "        γ: float,\n",
    "        approx_0: FunctionApprox[S]\n",
    ") -> Iterator[FunctionApprox[S]]:\n",
    "\n",
    "    '''Iteratively calculate the value function for the give finite Markov\n",
    "    Reward Process, using the given FunctionApprox to approximate the\n",
    "    value function at each step.\n",
    "\n",
    "    '''\n",
    "    def update(v: FunctionApprox[S]) -> FunctionApprox[S]:\n",
    "        vs: np.ndarray = v.evaluate(mrp.non_terminal_states)\n",
    "        updated: np.ndarray = mrp.reward_function_vec + γ * \\\n",
    "            mrp.get_transition_matrix().dot(vs)\n",
    "        return v.update(zip(mrp.states(), updated))\n",
    "\n",
    "    return iterate(update, approx_0)\n",
    "\n",
    "\n",
    "def policy_iteration_finite(\n",
    "    mdp: FiniteMarkovDecisionProcess[S, A],\n",
    "     γ: float,\n",
    "    approx_0: FunctionApprox[S]\n",
    ") -> Iterator[FunctionApprox[S]]:\n",
    "    '''Perform approximate policy iteration using the given FunctionApprox to approximate the\n",
    "    Optimal Value function at each step\n",
    "\n",
    "    '''\n",
    "    tol:float = 1.e-2\n",
    "    \n",
    "    \n",
    "    def update(v: FunctionApprox[S]) -> FunctionApprox[S]:\n",
    "        policy: FinitePolicy[S,A] = FinitePolicy({s:Choose({k for k in a.keys()}) for s,a in mdp.mapping if (not a is None)})\n",
    "    \n",
    "        # perform policy evaluation\n",
    "        eval_it = evaluate_finite_mrp(mdp.apply_finite_policy(policy), γ, approx_0)\n",
    "        old_vf = eval_it.__next__()\n",
    "        while(True):\n",
    "            new_vf = eval_it.__next__()\n",
    "            if (all([np.abs(old_vf[i] - new_vf[i]) < tol for i in range(len(old_vf))])):\n",
    "                break\n",
    "            old_vf = new_vf\n",
    "        \n",
    "        # greedy policy improvement\n",
    "        poli\n",
    "        \n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
